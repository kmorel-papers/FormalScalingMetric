% -*- latex -*-

\documentclass[conference]{IEEEtran}

\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage[cmex10]{amsmath}
\usepackage{booktabs}
%\usepackage{enumitem}
\usepackage{graphicx}
\usepackage{fancyvrb}
\usepackage{ifthen}
\usepackage{cite}
\usepackage[caption=false,font=footnotesize]{subfig}
\usepackage{tabulary}
\usepackage{url}
\usepackage{xspace}
\usepackage[pdfborder={0 0 0}]{hyperref}
\usepackage{verbatim}

\usepackage{color}
\definecolor{yellow}{rgb}{1,1,0}
\definecolor{black}{rgb}{0,0,0}
\definecolor{ltcyan}{rgb}{.75,1,1}
\definecolor{red}{rgb}{1,0,0}
\definecolor{gray}{rgb}{.6,.6,.6}
\definecolor{darkred}{rgb}{0.5,0,0}
\definecolor{darkgreen}{rgb}{0,0.5,0}

% Cite commands I use to abstract away the different ways to reference an
% entry in the bibliography (superscripts, numbers, dates, or author
% abbreviations).  \scite is a short cite that is used immediately after
% when the authors are mentioned.  \lcite is a full citation that is used
% anywhere.  Both should be used right next to the text being cited without
% any spacing.
\newcommand*{\lcite}[1]{~\cite{#1}}
\newcommand*{\scite}[1]{~\cite{#1}}

\newcommand{\etal}{et al.}

\newcommand*{\keyterm}[1]{\emph{#1}}

\newcommand{\fix}[1]{{\color{red}\textsc{[#1]}}}

% Avoid putting figures on their own page.
\renewcommand{\textfraction}{0.05}
\renewcommand{\topfraction}{0.95}
\renewcommand{\bottomfraction}{0.95}

% Make sure this is big enough so that only big figures end up on their own
% page but small enough so that if a figure does have to be on its own
% page, it won't push everything to the bottom because it's not big enough
% to have its own page.
\renewcommand{\floatpagefraction}{.75}

\newenvironment{packed_itemize}{
  \begin{itemize}[noitemsep]
}{
  \end{itemize}
}


\begin{document}

\sloppy

%
% paper title
% can use linebreaks \\ within to get better formatting as desired
\title{A Formal Metric for Large-Scale Parallel Algorithm Performance Analysis}


% author names and affiliations
% use a multiple column layout for up to three different
% affiliations
%% \author{\IEEEauthorblockN{Michael Shell}
%% \IEEEauthorblockA{School of Electrical and\\Computer Engineering\\
%% Georgia Institute of Technology\\
%% Atlanta, Georgia 30332--0250\\
%% Email: http://www.michaelshell.org/contact.html}
%% \and
%% \IEEEauthorblockN{Homer Simpson}
%% \IEEEauthorblockA{Twentieth Century Fox\\
%% Springfield, USA\\
%% Email: homer@thesimpsons.com}
%% \and
%% \IEEEauthorblockN{James Kirk\\ and Montgomery Scott}
%% \IEEEauthorblockA{Starfleet Academy\\
%% San Francisco, California 96678-2391\\
%% Telephone: (800) 555--1212\\
%% Fax: (888) 555--1212}}

\author{
  \IEEEauthorblockN{Kenneth Moreland}
  \IEEEauthorblockA{
    Sandia National Laboratories\\
    Email: kmorel@sandia.gov}
}

% conference papers do not typically use \thanks and this command
% is locked out in conference mode. If really needed, such as for
% the acknowledgment of grants, issue a \IEEEoverridecommandlockouts
% after \documentclass

% for over three affiliations, or if they all won't fit within the width
% of the page, use this alternative format:
% 
%\author{\IEEEauthorblockN{Michael Shell\IEEEauthorrefmark{1},
%Homer Simpson\IEEEauthorrefmark{2},
%James Kirk\IEEEauthorrefmark{3}, 
%Montgomery Scott\IEEEauthorrefmark{3} and
%Eldon Tyrell\IEEEauthorrefmark{4}}
%\IEEEauthorblockA{\IEEEauthorrefmark{1}School of Electrical and Computer Engineering\\
%Georgia Institute of Technology,
%Atlanta, Georgia 30332--0250\\ Email: see http://www.michaelshell.org/contact.html}
%\IEEEauthorblockA{\IEEEauthorrefmark{2}Twentieth Century Fox, Springfield, USA\\
%Email: homer@thesimpsons.com}
%\IEEEauthorblockA{\IEEEauthorrefmark{3}Starfleet Academy, San Francisco, California 96678-2391\\
%Telephone: (800) 555--1212, Fax: (888) 555--1212}
%\IEEEauthorblockA{\IEEEauthorrefmark{4}Tyrell Inc., 123 Replicant Street, Los An3geles, California 90210--4321}}


\maketitle


\begin{abstract}
Performance measurement of parallel algorithms is well studied and well
understood. However, a flaw in our performance metrics is that they rely on
comparisons to serial performance with the same input. This comparison is
convenient for theoretic complexity analysis but impossible to perform in
large-scale empirical studies with data sizes far too large to run on a
single serial computer.  Consequently, scaling studies currently rely ad
hoc methods that, although effective, have no grounded mathematical models.
In this position paper we advocate using a rate-based model that has a
concrete meaning relative to speedup and efficiency and that can be used to
unify strong and weak scaling studies.
\end{abstract}

\section{Introduction}

\noindent
The theory of parallel performance is well studied and often used in
practice, yet all our current models are limited by their reliance on
serial behavior.

\subsection{Performance Analysis Theory}

\noindent
The \keyterm{speedup} of a parallel algorithm is defined as
\begin{equation}
  S(n,p) = \frac{T^*(n)}{T(n,p)}
  \label{eq:speedup}
\end{equation}
where $T(n,p)$ is the time it takes to run the parallel algorithm on $p$
processing elements with an input of size $n$ and $T^*(n)$ the time for the
best serial algorithm on the same input. The best possible serial algorithm
may be different than the parallel algorithm although using the same
algorithm is also common practice.

In theory the best possible speedup achievable\lcite{Faber1986} is $S(n,p)
= p$ (although in practice superlinear measurements are possible). Thus, we
measure the \keyterm{efficiency} as the ratio of the observed speedup to
the ideal speedup.
\begin{equation}
  E(n,p) = \frac{S(n,p)}{p} = \frac{T^*(n)}{p \; T(n,p)}
  \label{eq:efficiency}
\end{equation}

Amdahl\lcite{Amdahl1967} famously observes the limits of scaling any
parallel algorithm based on the computation fraction $f$ that exists in any
algorithm that is inherently serial. The equation derived from this
observation is known as \keyterm{Amdahl's Law}.
\begin{equation}
  S(n,p) \leq \frac{1}{f + (1-f)/p}
  \label{eq:Amdahl}
\end{equation}

Gustafson\lcite{Gustafson1988} observes that the serail fraction tends to
go down for larger data sizes in parallel algorithms, which justifies the
use of parallel computing for large problems. The Gustafson-Barsis law
reformulates speedup in terms of the parallel execution rather than the
serial execution
\begin{equation}
  S(n,p) \leq p + f(1-p)
  \label{eq:GustafsonBarsis}
\end{equation}
This law shows that speedup can be increased indefinitely as
long as the serial fraction drops commensurately with the processing
element increase. Grama \etal\lcite{Grama1993} introduce an
\keyterm{isoefficiency} metric that determines how much a problem needs to
grow to maintain linear speedup.

Performance analysis theory is reviewed in much more detail in many
parallel computing textbooks such as Quinn's\lcite{Quinn2004}.

\subsection{Limitations of Performance Analysis}


% use section* for acknowledgement
\section*{Acknowledgment}

\noindent
Sandia National Laboratories is a multi-program laboratory managed and
operated by Sandia Corporation, a wholly owned subsidiary of Lockheed
Martin Corporation, for the U.S. Department of Energy's National Nuclear
Security Administration under contract DE-AC04-94AL85000.

\bibliographystyle{IEEETranS}
\bibliography{FormalScalingMetric}

\end{document}


