%% This BibTeX bibliography file was created using BibDesk.
%% http://bibdesk.sourceforge.net/


%% Created for kmorel at 2015-05-18 09:57:18 -0600 


%% Saved with string encoding Unicode (UTF-8) 



@inproceedings{Gustafson1990,
	Abstract = {In the problem size-ensemble size plane, fixed-sized and scaled-sized paradigms have been the subsets of primary interest to the parallel processing community. A problem with the newer scaled-sized model is that execution time increases for problems where operation complexity grows faster than storage complexity. The fixed time model is introduced, which, unlike the scaled model, implies the need to reduce problem size per processor. This reduction causes uniprocessor speed to vary. Historical ensemble models hold uniprocessor performance flat as problem size varies, even beyond physical memory size. However, tiered memory can make performance increase instead of decrease as problem size per processor shrinks, and workload can shift to routines with higher speed as the problem is scaled. Superlinear speedup results in such cases. Superlinear speedup, far from being an anomaly, becomes commonplace when the performance model makes realistic assumptions about memory speed and problem scaling.},
	Annote = {A nice paper on the possibility of superlinear speedup in parallel computing in practice. Over 25 years old but still feels relevent.

Another interesting aspect is that Gustafson defines speedup as the rate of the parallel version over the rate of the serial version. This makes it more feasible to compare to serial performance (especially in 1990).},
	Author = {John L. Gustafson},
	Booktitle = {Proceedings of the Fifth Distributed Memory Computing Conference},
	Date-Added = {2015-02-11 20:56:58 +0000},
	Date-Modified = {2015-02-11 20:56:58 +0000},
	Month = {April},
	Note = {{DOI}~10.1109/DMCC.1990.556383},
	Pages = {1255--1260},
	Title = {Fixed Time, Tiered Memory, and Superlinear Speedup},
	Year = {1990},
	Bdsk-Url-1 = {http://hint.byu.edu/documentation/Gus/Superlinear/Superlinear.html},
	Bdsk-Url-2 = {http://dx.doi.org/10.1109/DMCC.1990.556383}}

@book{Cleveland1994,
	Annote = {An old but applicable textbook on charting data. Is the source of a nice example from [Tufte1997] that shows how changing the scaling of a chart has a dramatic effect on the interpretation of the data.

I actually don't have a copy of this book. Prehaps I should get one. I did see some abbriviated extracts and found the reference to the chart in question.
},
	Author = {William S. Cleveland},
	Date-Added = {2015-01-26 23:47:05 +0000},
	Date-Modified = {2015-01-26 23:47:05 +0000},
	Edition = {2},
	Note = {{ISBN}~978-0963488411},
	Publisher = {Hobart Press},
	Title = {The Elements of Graphing Data},
	Year = {1994}}

@inproceedings{Oldfield2014,
	Abstract = {Exascale supercomputing will embody many revolutionary changes in the hardware and software of high-performance computing. For example, projected limitations in power and I/O-system performance will fundamentally change visualization and analysis workflows. A traditional post-processing workflow involves storing simulation results to disk and later retrieving them for visualization and data analysis; however, at Exascale, post-processing approaches will not be able to capture the volume or granularity of data necessary for analysis of these extreme-scale simulations. As an alternative, researchers are exploring ways to integrate analysis and simulation without using the storage system. In situ and in transit are two options, but there has not been an adequate evaluation of these approaches to identify strengths, weaknesses, and trade-offs at large scale. This paper provides a detailed performance and scaling analysis of a large-scale shock physics code using traditional post-processsing, in situ, and in transit analysis to detect material fragments from a simulated explosion.},
	Annote = {Follow up paper for the studies given in the technical report of [Rogers2013].},
	Author = {Ron A. Oldfield and Kenneth Moreland and Nathan Fabian and David Rogers},
	Booktitle = {Proceedings of the ICS '14},
	Date-Added = {2014-07-03 23:49:14 +0000},
	Date-Modified = {2015-05-18 15:56:33 +0000},
	Month = {June},
	Note = {{DOI}~10.1145/2597652.2597668},
	Pages = {83--92},
	Title = {Evaluation of Methods to Integrate Analysis into a Large-Scale Shock Physics Code},
	Year = {2014},
	Bdsk-Url-1 = {http://dx.doi.org/10.1145/2597652.2597668}}

@techreport{Rogers2013,
	Abstract = {Exascale supercomputing will embody many revolutionary changes in the hardware and software of high-performance computing. A particularly pressing issue is gaining insight into the science behind the exascale computations. Power and I/O speed con- straints will fundamentally change current visualization and analysis workflows. A traditional post-processing workflow involves storing simulation results to disk and later retrieving them for visualization and data analysis. However, at exascale, scien- tists and analysts will need a range of options for moving data to persistent storage, as the current offline or post-processing pipelines will not be able to capture the data necessary for data analysis of these extreme scale simulations. This Milestone explores two alternate workflows, characterized as in situ and in transit, and compares them. We find each to have its own merits and faults, and we provide information to help pick the best option for a particular use.},
	Annote = {Our report for the ASC II milestone for large scale in situ processing with CTH. This reference should be replaced with a published version once available.},
	Author = {David Rogers and Kenneth Moreland and Ron Oldfield and Nathan Fabian},
	Date-Added = {2014-07-03 23:40:43 +0000},
	Date-Modified = {2014-07-03 23:40:43 +0000},
	Institution = {Sandia National Laboratories},
	Number = {SAND2013-1122},
	Title = {Data Co-Processing for Extreme Scale Analysis Level {II} {ASC} Milestone (4745)},
	Year = {2013}}

@inproceedings{Rossinelli2013,
	Abstract = {We present unprecedented, high throughput simulations of cloud cavitation collapse on 1.6 million cores of Sequoia reaching 55% of its nominal peak performance, corresponding to 11 PFLOP/s. The destructive power of cavitation reduces the lifetime of energy critical systems such as internal combustion engines and hydraulic turbines, yet it has been harnessed for water purification and kidney lithotripsy. The present two-phase flow simulations enable the quantitative prediction of cavitation using 13 trillion grid points to resolve the collapse of 15'000 bubbles. We advance by one order of magnitude the current state-of-the-art in terms of time to solution, and by two orders the geometrical complexity of the flow. The software successfully addresses the challenges that hinder the effective solution of complex flows on contemporary supercomputers, such as limited memory bandwidth, I/O bandwidth and storage capacity. The present work redefines the frontier of high performance computing for fluid dynamics simulations.},
	Annote = {2013 Gordon Bell prize winner. Largest simulation was 11 petaflops on a 13 trillion point grid.},
	Author = {Diego Rossinelli and others},
	Booktitle = {Proceedings of the SC '13},
	Date-Added = {2014-07-02 21:15:24 +0000},
	Date-Modified = {2015-05-18 15:57:11 +0000},
	Month = {November},
	Note = {{DOI}~10.1145/2503210.2504565},
	Title = {11 {PFLOP/s} Simulations of Cloud Cavitation Collapse},
	Year = {2013},
	Bdsk-Url-1 = {http://dx.doi.org/10.1145/2503210.2504565}}

@inproceedings{Habib2013,
	Abstract = {Supercomputing is evolving towards hybrid and accelerator-based architectures with millions of cores. The HACC (Hardware/Hybrid Accelerated Cosmology Code) framework exploits this diverse landscape at the largest scales of problem size, obtaining high scalability and sustained performance. Developed to satisfy the science requirements of cosmological surveys, HACC melds particle and grid methods using a novel algorithmic structure that flexibly maps across architectures, including CPU/GPU, multi/many-core, and Blue Gene systems. We demonstrate the success of HACC on two very different machines, the CPU/GPU system Titan and the BG/Q systems Sequoia and Mira, attaining unprecedented levels of scalable performance. We demonstrate strong and weak scaling on Titan, obtaining up to 99.2% parallel efficiency, evolving 1.1 trillion particles. On Sequoia, we reach 13.94 PFlops (69.2% of peak) and 90% parallel efficiency on 1,572,864 cores, with 3.6 trillion particles, the largest cosmological benchmark yet performed. HACC design concepts are applicable to several other supercomputer applications.},
	Annote = {2013 Gordon Bell prize finalist. Largest simulation was 13.94 petaflops for 3.6 trillion particles.},
	Author = {Salman Habib and others},
	Booktitle = {Proceedings of the SC '13},
	Date-Added = {2014-07-02 21:15:16 +0000},
	Date-Modified = {2015-05-18 15:55:33 +0000},
	Month = {November},
	Note = {{DOI}~10.1145/2503210.2504566},
	Title = {{HACC}: Extreme Scaling and Performance Across Diverse Architectures},
	Year = {2013},
	Bdsk-Url-1 = {http://dx.doi.org/10.1145/2503210.2504566}}

@inproceedings{Bussmann2013,
	Abstract = {We present a particle-in-cell simulation of the relativistic Kelvin-Helmholtz Instability (KHI) that for the first time delivers angularly resolved radiation spectra of the particle dynamics during the formation of the KHI. This enables studying the formation of the KHI with unprecedented spatial, angular and spectral resolution. Our results are of great importance for understanding astrophysical jet formation and comparable plasma phenomena by relating the particle motion observed in the KHI to its radiation signature.

The innovative methods presented here on the implementation of the particle-in-cell algorithm on graphic processing units can be directly adapted to any many-core parallelization of the particle-mesh method. With these methods we see a peak performance of 7.176 PFLOP/s (double-precision) plus 1.449 PFLOP/s (single-precision), an efficiency of 96% when weakly scaling from 1 to 18432 nodes, an efficiency of 68.92% and a speed up of 794 (ideal: 1152) when strongly scaling from 16 to 18432 nodes.},
	Annote = {2013 Gordon Bell prize finallist. Peak performance of 7 petaflops on a 4.7 billion point grid with 75 billion particles.},
	Author = {M. Bussmann and others},
	Booktitle = {Proceedings of the SC 2013},
	Date-Added = {2014-07-02 21:15:06 +0000},
	Date-Modified = {2015-05-18 15:51:30 +0000},
	Month = {November},
	Note = {{DOI}~10.1145/2503210.2504564},
	Title = {Radiative Signatures of the Relativistic {Kelvin-Helmholtz} Instability},
	Year = {2013},
	Bdsk-Url-1 = {http://dx.doi.org/10.1145/2503210.2504564}}

@inproceedings{Bernaschi2013,
	Abstract = {We present performance results for the simulation of proteins suspensions in crowding conditions obtained with MUPHY, a computational platform for multi-scale simulations of real-life biofluidic problems. Previous versions of MUPHY have been used in the past for the simulation of blood flow through the human coronary arteries and DNA translocation across nanopores. The simulation exhibits excellent scalability up to 18, 000 K20X Nvidia GPUs and achieves almost 20 Petaflops of aggregate sustained performance with a peak performance of 27.5 Petaflops for the most intensive computing component. Those figures demonstrate once again the flexibility of MUPHY in simulating biofluidic phenomena, exploiting at their best the features of the architecture in use. Preliminary results were obtained in the present case on a completely different platform, the IBM Blue Gene/Q. The combination of novel mathematical models, computational algorithms, hardware technology, code tuning and parallelization techniques required to achieve these results are presented.},
	Annote = {Gordon Bell finalist. Largest simulation was of 18,000 proteins in an 18 billion point grid. Demonstrates an aggregate performance of 27.4 petaflops.},
	Author = {Massimo Bernaschi and Mauro Bisson and Massimiliano Fatica and Simone Melchionna},
	Booktitle = {Proceedings of the SC 2013},
	Date-Added = {2014-07-02 21:15:05 +0000},
	Date-Modified = {2015-05-18 15:51:37 +0000},
	Month = {November},
	Note = {{DOI}~10.1145/2503210.2504563},
	Title = {20 Petaflops Simulation of Proteins Suspensions in Crowding Conditions},
	Year = {2013},
	Bdsk-Url-1 = {http://dx.doi.org/10.1145/2503210.2504563}}

@article{Cameron2012,
	Abstract = {Extending Amdahl's law to identify optimal power-performance configurations requires considering the interactive effects of power, performance, and parallel overhead.},
	Annote = {A very high level paper describing some work in generalizing the ideas of Amdahl's law to take into account the amount of power spent in the parallel job.  Basically a power parameter is added to the time functions in the definition of speedup.},
	Author = {Kirk W. Cameron and Rong Ge},
	Date-Added = {2014-07-01 23:08:50 +0000},
	Date-Modified = {2015-05-18 15:53:58 +0000},
	Journal = {IEEE Computer},
	Month = {March},
	Note = {{DOI}~10.1109/MC.2012.92},
	Number = {3},
	Pages = {75--77},
	Title = {Generalizing {Amdahl's} Law for Power and Energy},
	Volume = {45},
	Year = {2012},
	Bdsk-Url-1 = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?reload=true&arnumber=6163449},
	Bdsk-Url-2 = {http://dx.doi.org/10.1109/MC.2012.92}}

@book{Kaminsky2015,
	Annote = {A textbook on parallel program and performance analysis. At this time (July 2014), the book is not quite complete but is made available through the Creative Commons Attribution license.

This is so far the only source I have found to define strong and weak scaling (although many publications use these terms). Interestingly, it also uses rate as opposed to time as the fundamental measurement.},
	Author = {Alan Kaminsky},
	Date-Added = {2014-07-01 21:09:17 +0000},
	Date-Modified = {2015-01-29 06:34:17 +0000},
	Note = {Retrieved from \url{http://www.cs.rit.edu/~ark/bcbd}},
	Publisher = {Unpublished manuscript},
	Title = {Big CPU, Big Data: Solving the World's Toughest Computational Problems with Parallel Computing},
	Year = {2015},
	Bdsk-Url-1 = {http://www.cs.rit.edu/~ark/bcbd/}}

@book{JaJa1992,
	Annote = {A textbook on designing parallel algorithms. I have a particular affinity for this book because it was my textbook for my second parallel algorithms class, where I actually "got it" in the sense of being able to apply the teachings to create real algorithms. The textbook advocates using some basic parallel techniques to build larger algorithms (rather than try to design everything from scratch). I also see it referenced by other literature relatively frequently.},
	Author = {Joseph J\'{a}J\'{a}},
	Date-Added = {2014-07-01 17:28:24 +0000},
	Date-Modified = {2014-07-01 17:28:24 +0000},
	Note = {{ISBN}~0-201-54856-9},
	Publisher = {Addison Wesley},
	Title = {An Introduction to Parallel Algorithms},
	Year = {1992}}

@article{Karp1990,
	Abstract = {Many metrics are used for measuring the performance of a parallel algorithm running on a parallel processor. This article introduces a new metric that has some advantages over the others. Its use is illustrated with data from the Linpack benchmark report and the winners of the Gordon Bell Award.},
	Annote = {Source for the Karp-Flatt metric to experimentally estimate the serial fraction of a parallel algorithm. The Karp-Flatt metric is reviewed in [Quinn2004].},
	Author = {Alan H. Karp and Horace P. Flatt},
	Date-Added = {2014-06-30 21:18:30 +0000},
	Date-Modified = {2014-06-30 21:18:30 +0000},
	Journal = {Communications of the ACM},
	Month = {May},
	Note = {{DOI}~10.1145/78607.78614},
	Number = {5},
	Pages = {539--543},
	Title = {Measuring Parallel Processor Performance},
	Volume = {33},
	Year = {1990},
	Bdsk-Url-1 = {http://dx.doi.org/10.1145/78607.78614}}

@book{Quinn2004,
	Annote = {A book containing mostly practical algorithms and their implementations using MPI and OpenMP.  The book also contains what I think is a nice review of some of the basic theory of parallel computing including Amdahl's law, Gustafson-Barsis's law, the Karp-Flatt metric, and the Isoefficiency metric.},
	Author = {Michael J. Quinn},
	Date-Added = {2014-06-30 19:01:50 +0000},
	Date-Modified = {2014-06-30 19:01:50 +0000},
	Note = {{ISBN}~978-0-07-282256-4},
	Publisher = {McGraw-Hill},
	Title = {Parallel Programming in C with MPI and OpenMP},
	Year = {2004}}

@article{Grama1993,
	Abstract = {Isoefficiency analysis helps us determine the best algorithm/architecture combination for a particular problem without explicitly analyzing all possible combinations under all possible conditions.},
	Annote = {Introduces an isoefficiency metric that determines how much you have to increase a problem size to maintain the same efficiency for a larger problem.

Summarized in [Quinn2004].
},
	Author = {Ananth Y. Grama and Anshul Gupta and Vipin Kuma},
	Date-Added = {2014-06-30 18:57:53 +0000},
	Date-Modified = {2014-06-30 18:57:53 +0000},
	Journal = {IEEE Parallel \& Distributed Technology: Systems \& Applications},
	Month = {August},
	Note = {{DOI}~10.1109/88.242438},
	Number = {3},
	Pages = {12--21},
	Title = {Isoefficiency: Measuring the Scalability of Parallel Algorithms and Architectures},
	Volume = {1},
	Year = {1993},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/88.242438}}

@article{Gustafson1988,
	Annote = {A paper by Gustafson describing how efficient parallel processing can be achieved by scalling up the size of the problem with the amount of parallelism.  This is in contrast to Amdahl's law, which limits the amount of speedup that can be achieved.  Provides the equation that is now known as Gustafson's law (or Gustafson-Barsis's law).},
	Author = {John L. Gustafson},
	Date-Added = {2014-06-30 18:26:15 +0000},
	Date-Modified = {2014-06-30 18:26:15 +0000},
	Journal = {Communications of the ACM},
	Month = {May},
	Note = {{DOI}~10.1145/42411.42415},
	Number = {5},
	Pages = {532--533},
	Title = {Reevaluating {Amdahl's} Law},
	Volume = {31},
	Year = {1988},
	Bdsk-Url-1 = {http://dx.doi.org/10.1145/42411.42415}}

@inproceedings{Amdahl1967,
	Annote = {Amdahl's original (?) short paper describing the limitations of amount of increased speedup you get with parallel computing.  I note that the equation now associated with Amdahl's law is not given here.},
	Author = {Gene M. Amdahl},
	Booktitle = {Proceedings of the AFIPS '67},
	Date-Added = {2014-06-30 18:14:06 +0000},
	Date-Modified = {2015-05-18 15:49:49 +0000},
	Month = {April},
	Note = {{DOI}~10.1145/1465482.1465560},
	Pages = {483--485},
	Title = {Validity of the single processor approach to achieving large scale computing capabilities},
	Year = {1967},
	Bdsk-Url-1 = {http://dx.doi.org/10.1145/1465482.1465560}}

@article{Faber1986,
	Annote = {Paper demonstrating that in theory no parallel algorithm can have better than linear performance. The short (and in retrospect obvious) proof is that if a parallel algorithm has a speedup greater than p (the number of processing elements), then you could make a serial algorithm that ran the instructions for each of the p processing elements in sequence, and that would take p times the time of the serial algorithm, which would result in a speedup of p.

Of course, in practice it is possible to get superlinear speedup if the problem fits better in smaller parallel units.
},
	Author = {V Faber and Olaf M. Lubeck and Andrew B. {White Jr.}},
	Date-Added = {2014-06-30 18:02:38 +0000},
	Date-Modified = {2015-05-18 15:53:52 +0000},
	Journal = {Parallel Computing},
	Month = {July},
	Note = {{DOI}~10.1016/0167-8191(86)90024-4},
	Number = {3},
	Pages = {259--260},
	Title = {Superlinear speedup of an efficient sequential algorithm is not possible},
	Volume = {3},
	Year = {1986},
	Bdsk-Url-1 = {http://dx.doi.org/10.1016/0167-8191(86)90024-4}}
